{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet results classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transform all the data, we saved the data for all the subjects, for each animal, for each level, and variance for each orientation. Since the statistical testing didn't work out well, now we are going to do the classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VT classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1 load data (VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import sklearn\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load in the final data: all subject, all animals, all levels, all orientations;\n",
    "data=np.load('data_all_subjects_final_VT.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1230, 5, 28, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1230 is all TRs, 5 is 5 levels, 28 orientation, 18 subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1230)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in all the labels/conditions\n",
    "\n",
    "conditions=loadmat('all_subjects_18/regressors_sh3.mat')\n",
    "conditions = conditions['regressors_sh3']\n",
    "conditions.shape #12 labels, 1230trs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 is 12 animals, 1230 TRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  8.,  8.,  8.,  8.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  7.,  7.,  7.,  7.,  0.,  0.,  0.,  0.,  0.,  0., 11.,\n",
       "       11., 11., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  9.,  9.,  9.,  9.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  5.,  5.,  5.,  5.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0., 10., 10., 10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,  3.,  3.,\n",
       "        3.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  2.,  2.,  2.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0., 12., 12., 12., 12.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  4.,  4.,  4.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  6.,  6.,\n",
       "        6.,  6.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_binary_to_multiclass(binary_conditions):\n",
    "    \"\"\"Convert binary representation into multiclass reprentation:\n",
    "    For example: convert [[1 1 1 1 0 0 0 0]\n",
    "                          [0 0 0 0 1 1 1 1]]\n",
    "    to [1 1 1 1 2 2 2 2]\"\"\"\n",
    "    x,y = np.where(binary_conditions)\n",
    "    conditions=np.zeros(binary_conditions.shape[1])\n",
    "    conditions[y]=x+1\n",
    "    return conditions\n",
    "\n",
    "conditions_multi = convert_binary_to_multiclass(conditions)\n",
    "conditions_multi[:123]#first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I label all the conditions\n",
    "insect1 = conditions_multi == 1\n",
    "insect2 = conditions_multi == 2\n",
    "insect3 = conditions_multi == 3\n",
    "insect4 = conditions_multi == 4\n",
    "bird1 = conditions_multi == 5\n",
    "bird2 = conditions_multi == 6\n",
    "bird3 = conditions_multi == 7\n",
    "bird4 = conditions_multi == 8\n",
    "monkey1 = conditions_multi == 9\n",
    "monkey2 = conditions_multi == 10\n",
    "monkey3 = conditions_multi == 11\n",
    "monkey4 = conditions_multi == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1230, 5, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,:,:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to isoldate each level because we want to compare the accuracy across levels. For each levels, the features would be 28 orientation values (variance). \n",
    "\n",
    "How to deal with all the subjects? First we will try treat all the subjects equally without regarding their differences. So we will use all the subjects data. \n",
    "\n",
    "How to deal with all the TRs? We can't avarage all the TRs because it doesn't make sense. We will keep all the TRs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data frame transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data, we will need 1230x18(rows), 28 (features; columns), 1230x18 (identities;rows);\n",
    "\n",
    "The shape of the feature matrix is (1230x18, 28); The shape of the label matrix is (1230x18,1): 1 is for each animal identification.\n",
    "\n",
    "Nest step is to transform the data based on above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1230, 28, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, let's work on the level1, but save separately the data on each level.\n",
    "\n",
    "L1= data[:,0,:,:]\n",
    "L2= data[:,1,:,:]\n",
    "L3= data[:,2,:,:]\n",
    "L4= data[:,3,:,:]\n",
    "L5= data[:,4,:,:]\n",
    "\n",
    "L1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj=[]\n",
    "for i in range(18):\n",
    "    subj_data=L1[:,:,i]\n",
    "    subj.append(subj_data)\n",
    "L1_features=np.vstack(subj)\n",
    "\n",
    "subj2=[]\n",
    "for i in range(18):\n",
    "    subj2_data=L2[:,:,i]\n",
    "    subj2.append(subj2_data)\n",
    "L2_features=np.vstack(subj2)\n",
    "\n",
    "subj3=[]\n",
    "for i in range(18):\n",
    "    subj3_data=L3[:,:,i]\n",
    "    subj3.append(subj3_data)\n",
    "L3_features=np.vstack(subj3)\n",
    "\n",
    "subj4=[]\n",
    "for i in range(18):\n",
    "    subj4_data=L4[:,:,i]\n",
    "    subj4.append(subj4_data)\n",
    "L4_features=np.vstack(subj4)\n",
    "\n",
    "subj5=[]\n",
    "for i in range(18):\n",
    "    subj5_data=L5[:,:,i]\n",
    "    subj5.append(subj5_data)\n",
    "L5_features=np.vstack(subj5)\n",
    "\n",
    "#L1_new=np.reshape(L1,(1230*18,28))\n",
    "#L1_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22140, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_features.shape #It is the shape we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, work on the labels matrix\n",
    "conditions_multi=conditions_multi.reshape(1230,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list=[]\n",
    "for i in range(18):\n",
    "    labels_list.append(conditions_multi)\n",
    "labels=np.vstack(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22140, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the features and labels are transformed as we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's get rid of resting state:\n",
    "#L1_features=L1_features[!=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_res=labels!=0\n",
    "no_res=no_res.reshape(1230*18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8640, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_no_res=labels[no_res]\n",
    "labels_no_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8640, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_features_no_res=L1_features[no_res,:]\n",
    "L2_features_no_res=L2_features[no_res,:]\n",
    "L3_features_no_res=L3_features[no_res,:]\n",
    "L4_features_no_res=L4_features[no_res,:]\n",
    "L5_features_no_res=L5_features[no_res,:]\n",
    "\n",
    "L2_features_no_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py2/lib/python2.7/site-packages/ipykernel_launcher.py:31: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06018518518518518, 0.08912037037037036, 0.0763888888888889, 0.07175925925925926, 0.09027777777777778, 0.06481481481481481, 0.09722222222222222, 0.08912037037037036, 0.09143518518518519, 0.0775462962962963]\n"
     ]
    }
   ],
   "source": [
    "# It seems works, lett's examine if the results is significant, so we will try cross-validation\n",
    "# from sklearn.model_selection import LeaveOneOut \n",
    "# LOOCV give too many folds, which is not adviable. Let's try kfolds=10. Marc's papers used this many folds.\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = L3_features_no_res\n",
    "y = labels_no_res\n",
    "\n",
    "#loo = LeaveOneOut()\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "#kf.get_n_splits(X)\n",
    "\n",
    "#print kf\n",
    "\n",
    "accuracy=[]\n",
    "\n",
    "for train_index, test_index in kf.split(X): \n",
    "    #print \"TRAIN:\", train_index, \"TEST:\", test_index \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #print X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    model = KNeighborsClassifier(n_neighbors=10)\n",
    "    \n",
    "    model.fit(X_train,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "    predicted = model.predict(X_test) # 0:Overcast, 2:Mild\n",
    "    acc=accuracy_score(y_test, predicted)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "print accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_value_kkn= -0.6448314199366041\n",
      "p_value_kkn= 0.5351255290357786\n"
     ]
    }
   ],
   "source": [
    "# Test the result significance\n",
    "from scipy import stats\n",
    "\n",
    "t_svm, p_svm= stats.ttest_1samp(accuracy,1./12)\n",
    "\n",
    "print 't_value_kkn=', t_svm\n",
    "print 'p_value_kkn=', p_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py2/lib/python2.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08564814814814815, 0.08796296296296297, 0.08449074074074074, 0.08217592592592593, 0.08333333333333333, 0.08912037037037036, 0.09953703703703703, 0.09606481481481481, 0.08449074074074074, 0.08680555555555555]\n"
     ]
    }
   ],
   "source": [
    "# Now let's try GNB classifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "X = L3_features_no_res\n",
    "y = labels_no_res\n",
    "\n",
    "#loo = LeaveOneOut()\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "#kf.get_n_splits(X)\n",
    "\n",
    "#print kf\n",
    "\n",
    "accuracy=[]\n",
    "\n",
    "for train_index, test_index in kf.split(X): \n",
    "    #print \"TRAIN:\", train_index, \"TEST:\", test_index \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #print X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    clf = GaussianNB()\n",
    "    \n",
    "    clf.fit(X_train,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "    predicted = clf.predict(X_test) # 0:Overcast, 2:Mild\n",
    "    acc=accuracy_score(y_test, predicted)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "print accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_value_gnb= 2.594026207002706\n",
      "p_value_gnb= 0.029021186366070362\n"
     ]
    }
   ],
   "source": [
    "t_gnb, p_gnb= stats.ttest_1samp(accuracy,1./12)\n",
    "\n",
    "print 't_value_gnb=', t_gnb\n",
    "print 'p_value_gnb=', p_gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py2/lib/python2.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08217592592592593, 0.08449074074074074, 0.08217592592592593, 0.08333333333333333, 0.08449074074074074]\n"
     ]
    }
   ],
   "source": [
    "# Let's try SVM \n",
    "\n",
    "from sklearn import svm\n",
    "    \n",
    "X = L2_features_no_res\n",
    "y = labels_no_res\n",
    "\n",
    "#loo = LeaveOneOut()\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "#kf.get_n_splits(X)\n",
    "\n",
    "#print kf\n",
    "\n",
    "accuracy=[]\n",
    "\n",
    "for train_index, test_index in kf.split(X): \n",
    "    #print \"TRAIN:\", train_index, \"TEST:\", test_index \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #print X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    clf = svm.SVC()\n",
    "    \n",
    "    clf.fit(X_train,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "    predicted = clf.predict(X_test) # 0:Overcast, 2:Mild\n",
    "    acc=accuracy_score(y_test, predicted)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "print accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_value_svm= 5.687550219094264e-14\n",
      "p_value_svm= 0.9999999999999558\n"
     ]
    }
   ],
   "source": [
    "t_svm, p_svm = stats.ttest_1samp(accuracy,1./12)\n",
    "\n",
    "print 't_value_svm=', t_svm\n",
    "print 'p_value_svm=', p_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1: p=0.5554454421055945;\n",
    "\n",
    "L2: p=0.912767280440428;\n",
    "\n",
    "L3: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "April 9 from last meeting:\n",
    "\n",
    "- Subjects run seperately;\n",
    "\n",
    "- Classification run pairwise, item level;\n",
    "\n",
    "- Leave one out measn leave one run out.\n",
    "\n",
    "Plan: \n",
    "\n",
    "- Do one subject first, then loop over all the subjects;\n",
    "- Focus on one level, then loop over all the levels. \n",
    "- Seperate all the animals and form all the pairs, choose one pair, eventually loop over all pairs;\n",
    "- Choose the runs according to the animal pairs, then leave one run for testing. Then shuffle the runs to do the cross-validation\n",
    "- Then we get the accuracy for each of the cross validation. Because we have 10 runs, so we have 10 accuracy scores. Then average over these 10 scores to give one single value. \n",
    "- Run all the loops above, generate one fime for repeated measure anova. \n",
    "- Stats part, run the repeated measure anova. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 0 \n",
    "\n",
    "level = 3\n",
    "\n",
    "pair =  [1,2] # represent animal 1 and animal 2 pair\n",
    "\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py2/lib/python2.7/site-packages/ipykernel_launcher.py:43: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "# First I will start with one subject. \n",
    "\n",
    "subj_data = data[:,:,:,subject] # First choose one subject data\n",
    "\n",
    "level_data = subj_data[:,level,:] # Then choose one level data\n",
    "\n",
    "#level_data.shape\n",
    "# choose the masks corresponding to each animal in the pair\n",
    "mask_1 = conditions_multi == pair[0]\n",
    "mask_2 = conditions_multi == pair[1]\n",
    "\n",
    "# Choose one run data according to the animal pair \n",
    "tr_index = np.arange(1230)\n",
    "# Each run has 123 TRs, to choose the first run, will use the logic below\n",
    "run_mask = np.logical_and(run*123 <= tr_index, tr_index < (run+1)*123)\n",
    "\n",
    "# Select the data by applying the mask\n",
    "pair_mask = np.logical_or(mask_1, mask_2)\n",
    "pair_mask = pair_mask.reshape(1230)\n",
    "\n",
    "test_mask = np.logical_and(pair_mask, run_mask) # choose the animal pair and one run data\n",
    "train_mask = np.logical_and(pair_mask,~run_mask) # ~ means invert mask, mean except that mask; choose the animal pairs and other 9 runs data\n",
    "\n",
    "test_data =  level_data[test_mask,:]\n",
    "train_data = level_data[train_mask,:]\n",
    "\n",
    "#Next generate the features matrix and label matrix to prep for the classification.\n",
    "labels_test = conditions_multi[test_mask] # choose the animal pair and run masks within all the conditions. test labels\n",
    "features_test = level_data [test_mask,:] # test features\n",
    "\n",
    "labels_train = conditions_multi[train_mask]\n",
    "features_train = level_data [train_mask,:]\n",
    "\n",
    "# Now we can run classification \n",
    "\n",
    "accuracy=[]\n",
    "\n",
    "X_train, X_test = features_train, features_test\n",
    "y_train, y_test = labels_train, labels_test\n",
    "# Train the model using the training sets\n",
    "#clf = svm.SVC()\n",
    "model = KNeighborsClassifier(n_neighbors=10)   \n",
    "model.fit(X_train,y_train)# model fit traning set.\n",
    "\n",
    "#Predict Output\n",
    "\n",
    "predicted = clf.predict(X_test) # 0:Overcast, 2:Mild\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "accuracy.append(acc)\n",
    "\n",
    "print accuracy\n",
    "\n",
    "#train_data.shape\n",
    "\n",
    "#now we have one run data for testing, we need 9 runs data set for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I turned above into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def wavefunc (subject, level, pair, run): \n",
    "    \"\"\"This function calculate each subject, each level, each animals pairs, and using leaving one run out cross-validation \n",
    "        method for classification. Subjects are from [0-18]; levels are from 0-4, animals pairs are from 1-12 (two numbers)\n",
    "        , run from 1-10. For example, wavefunc (0,3,[1,2],0)\"\"\"\n",
    "    # First I will start with one subject. \n",
    "\n",
    "    subj_data = data[:,:,:,subject] # First choose one subject data\n",
    "\n",
    "    level_data = subj_data[:,level,:] # Then choose one level data\n",
    "\n",
    "    #level_data.shape\n",
    "    # choose the masks corresponding to each animal in the pair\n",
    "    mask_1 = conditions_multi == pair[0]\n",
    "    mask_2 = conditions_multi == pair[1]\n",
    "\n",
    "    # Choose one run data according to the animal pair \n",
    "    tr_index = np.arange(1230)\n",
    "    # Each run has 123 TRs, to choose the first run, will use the logic below\n",
    "    run_mask = np.logical_and(run*123 <= tr_index, tr_index < (run+1)*123)\n",
    "\n",
    "    # Select the data by applying the mask\n",
    "    pair_mask = np.logical_or(mask_1, mask_2)\n",
    "    pair_mask = pair_mask.reshape(1230)\n",
    "\n",
    "    test_mask = np.logical_and(pair_mask, run_mask) # choose the animal pair and one run data\n",
    "    train_mask = np.logical_and(pair_mask,~run_mask) # ~ means invert mask, mean except that mask; choose the animal pairs and other 9 runs data\n",
    "\n",
    "    test_data =  level_data[test_mask,:]\n",
    "    train_data = level_data[train_mask,:]\n",
    "\n",
    "    #Next generate the features matrix and label matrix to prep for the classification.\n",
    "    labels_test = conditions_multi[test_mask] # choose the animal pair and run masks within all the conditions. test labels\n",
    "    features_test = log(level_data [test_mask,:]) # test features\n",
    "\n",
    "    labels_train = conditions_multi[train_mask]\n",
    "    features_train = log(level_data [train_mask,:])\n",
    "\n",
    "    # Now we can run classification\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train, X_test = features_train, features_test\n",
    "    y_train, y_test = labels_train, labels_test\n",
    "    # Train the model using the training sets\n",
    "    \n",
    "    X_train_scaler =  scaler.fit_transform (X_train)\n",
    "    X_test_scaler=  scaler.transform (X_test)\n",
    "    \n",
    "    model = svm.SVC()\n",
    "    #model = KNeighborsClassifier(n_neighbors=10)   \n",
    "    model.fit(X_train_scaler,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "\n",
    "    predicted = model.predict(X_test_scaler) # 0:Overcast, 2:Mild\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    \n",
    "    return acc\n",
    "    #train_data.shape\n",
    "\n",
    "    #now we have one run data for testing, we need 9 runs data set for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavefunc(0,3,[1,2],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5125\n"
     ]
    }
   ],
   "source": [
    "# Based on my function above, now I would like to run leave one run out cross-validation.\n",
    "acc = []\n",
    "for i in range(10):\n",
    "    acc.append(wavefunc(0,3,[1,2],i))\n",
    "\n",
    "acc =  np.asarray(acc)\n",
    "print acc.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to escalate things up to another whole level to update my function with cross-validation built-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunc_cross_validation (subject, level, pair):\n",
    "    \"\"\" This function is just the wavefunc with cross-validation built-in\"\"\"   \n",
    "    acc = []\n",
    "    for i in range(10):\n",
    "        acc.append(wavefunc(subject,level,pair,i))\n",
    "\n",
    "    acc =  np.asarray(acc)\n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavefunc_cross_validation(0,3,[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I would like to loop over all the subjects, all the levels, and all the animal pairs.\n",
    "\n",
    "subjects = np.arange(18) # 0-17 ; 18 subjects.\n",
    "\n",
    "levels = np.arange (5) # 0-4; 5 levels.\n",
    "\n",
    "animals = np.arange(1,13)\n",
    "\n",
    "# A Python program to print all \n",
    "# combinations of given list\n",
    "\n",
    "from itertools import combinations \n",
    "\n",
    "# Get all combinations of [1, 2, 3] \n",
    "# and length 2 \n",
    "animal_pairs = list(combinations(animals,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each subject:\n",
    "data_all = []\n",
    "\n",
    "for subject in subjects:\n",
    "    for level in levels:\n",
    "        for animal_pair in animal_pairs:\n",
    "            acc = wavefunc_cross_validation (subject, level, animal_pair)\n",
    "            if (animal_pair[0]-1)//4 == (animal_pair[1]-1)//4:\n",
    "                within = 1\n",
    "            else:\n",
    "                within = 0\n",
    "            result_dict = {\n",
    "                \"subject\": subject,\n",
    "                \"level\": level,\n",
    "                \"animal1\":animal_pair[0],\n",
    "                \"animal2\":animal_pair[1],\n",
    "                \"accuracy\":acc,\n",
    "                \"within\":within\n",
    "            }\n",
    "            data_all.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_all)\n",
    "df.to_csv(\"classification_VT_withinvscross_svc_logscale_scaler.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04/21 Since the pair-wise study didn't work out, I will try category study, which will be more powerful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 0 \n",
    "\n",
    "level = 3\n",
    "\n",
    "categories = [[1,2,3,4],[5,6,7,8],[9,10,11,12]] # represent all the animals in 3 categories\n",
    "\n",
    "category_pair = [0, 1]\n",
    "\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunc (subject, level, category_pair, run): \n",
    "    \"\"\"This function calculate each subject, each level, each category pairs, and using leaving one run out cross-validation \n",
    "        method for classification. Subjects are from [0-18]; levels are from 0-4, category pairs are from 0-2 (two numbers)\n",
    "        , run from 1-10. For example, wavefunc (0,3,[1,2],0)\"\"\"\n",
    "    # First I will start with one subject. \n",
    "\n",
    "    subj_data = data[:,:,:,subject] # First choose one subject data\n",
    "\n",
    "    level_data = subj_data[:,level,:] # Then choose one level data\n",
    "\n",
    "    #level_data.shape\n",
    "   # choose the masks corresponding to each category in the pair\n",
    "    mask_1 = np.in1d (conditions_multi, categories[category_pair[0]])# np.in1d you need to make sure what shape you want, the shape you want should be put as the first element.\n",
    "    mask_2 = np.in1d (conditions_multi, categories[category_pair[1]])\n",
    "    \n",
    "    # Choose one run data according to the animal pair \n",
    "    tr_index = np.arange(1230)\n",
    "    # Each run has 123 TRs, to choose the first run, will use the logic below\n",
    "    run_mask = np.logical_and(run*123 <= tr_index, tr_index < (run+1)*123)\n",
    "\n",
    "    # Select the data by applying the mask\n",
    "    category_mask = np.logical_or(mask_1, mask_2)\n",
    "    category_mask = category_mask.reshape(1230)\n",
    "\n",
    "    test_mask = np.logical_and(category_mask, run_mask) # choose the category pair and one run data\n",
    "    train_mask = np.logical_and(category_mask,~run_mask) # ~ means invert mask, mean except that mask; choose the animal pairs and other 9 runs data\n",
    "\n",
    "    test_data =  level_data[test_mask,:]\n",
    "    train_data = level_data[train_mask,:]\n",
    "\n",
    "    #Next generate the features matrix and label matrix to prep for the classification.\n",
    "    labels_test = conditions_multi[test_mask] # choose the category pair and run masks within all the conditions. test labels\n",
    "    features_test = level_data [test_mask,:] # test features\n",
    "\n",
    "    labels_train = conditions_multi[train_mask]\n",
    "    features_train = level_data [train_mask,:]\n",
    "\n",
    "    # Now we can run classification \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train, X_test = features_train, features_test\n",
    "    y_train, y_test = labels_train, labels_test\n",
    "    \n",
    "    X_train_scaler =  scaler.fit_transform (X_train)\n",
    "    X_test_scaler=  scaler.transform (X_test)\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    #model = svm.SVC()\n",
    "    #model = KNeighborsClassifier(n_neighbors=10) \n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train_scaler,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "\n",
    "    predicted = model.predict(X_test_scaler) # 0:Overcast, 2:Mild\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    \n",
    "    return acc\n",
    "    #train_data.shape\n",
    "\n",
    "    #now we have one run data for testing, we need 9 runs data set for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunc_cross_validation (subject, level, category_pair):\n",
    "    \"\"\" This function is just the wavefunc with cross-validation built-in\"\"\"   \n",
    "    acc = []\n",
    "    for i in range(10):\n",
    "        acc.append(wavefunc(subject,level,category_pair,i))\n",
    "\n",
    "    acc =  np.asarray(acc)\n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I would like to loop over all the subjects, all the levels, and all the category pairs.\n",
    "\n",
    "subjects = np.arange(18) # 0-17 ; 18 subjects.\n",
    "\n",
    "levels = np.arange (5) # 0-4; 5 levels.\n",
    "\n",
    "category_pair = np.arange(3)\n",
    "\n",
    "# A Python program to print all \n",
    "# combinations of given list\n",
    "\n",
    "from itertools import combinations \n",
    "\n",
    "# Get all combinations of [1, 2, 3] \n",
    "# and length 2 \n",
    "category_pairs = list(combinations(category_pair,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/envs/py2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-9946e470bc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcategory_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategory_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavefunc_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             result_dict = {\n",
      "\u001b[0;32m<ipython-input-93-a842042e14cd>\u001b[0m in \u001b[0;36mwavefunc_cross_validation\u001b[0;34m(subject, level, category_pair)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavefunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategory_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-3d42f17a1f48>\u001b[0m in \u001b[0;36mwavefunc\u001b[0;34m(subject, level, category_pair, run)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#model = KNeighborsClassifier(n_neighbors=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# model fit traning set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#Predict Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over each subject:\n",
    "data_all = []\n",
    "\n",
    "for subject in subjects:\n",
    "    for level in levels:\n",
    "        for category_pair in category_pairs:\n",
    "            acc = wavefunc_cross_validation(subject, level, category_pair)\n",
    "            \n",
    "            result_dict = {\n",
    "                \"subject\": subject,\n",
    "                \"level\": level,\n",
    "                \"category1\":category_pair[0],\n",
    "                \"category2\":category_pair[1],\n",
    "                \"accuracy\":acc,\n",
    "            }\n",
    "            data_all.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_all)\n",
    "df.to_csv(\"classification_Categorylevel_VT_svc_standardscaler.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04/24 Now I would like to try with all the aimals, no category, just make sure the wavelet works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 0 \n",
    "\n",
    "level = 3\n",
    "\n",
    "animals = [1,2,3,4,5,6,7,8,9,10,11,12] # represent all the animals in 3 categories\n",
    "\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunc (subject, level, animals, run): \n",
    "    \"\"\"This function calculate each subject, each level, each category pairs, and using leaving one run out cross-validation \n",
    "        method for classification. Subjects are from [0-18]; levels are from 0-4, category pairs are from 0-2 (two numbers)\n",
    "        , run from 1-10. For example, wavefunc (0,3,[1,2],0)\"\"\"\n",
    "    # First I will start with one subject. \n",
    "\n",
    "    subj_data = data[:,:,:,subject] # First choose one subject data\n",
    "\n",
    "    level_data = subj_data[:,level,:] # Then choose one level data\n",
    "\n",
    "    #level_data.shape\n",
    "   # choose the masks corresponding to each category in the pair\n",
    "    mask_1 = np.in1d (conditions_multi, animals)# np.in1d you need to make sure what shape you want, the shape you want should be put as the first element.\n",
    "    mask_2 = np.in1d (conditions_multi, animals)\n",
    "    \n",
    "    # Choose one run data according to the animal pair \n",
    "    tr_index = np.arange(1230)\n",
    "    # Each run has 123 TRs, to choose the first run, will use the logic below\n",
    "    run_mask = np.logical_and(run*123 <= tr_index, tr_index < (run+1)*123)\n",
    "\n",
    "    # Select the data by applying the mask\n",
    "    category_mask = np.logical_or(mask_1, mask_2)\n",
    "    category_mask = category_mask.reshape(1230)\n",
    "\n",
    "    test_mask = np.logical_and(category_mask, run_mask) # choose the category pair and one run data\n",
    "    train_mask = np.logical_and(category_mask,~run_mask) # ~ means invert mask, mean except that mask; choose the animal pairs and other 9 runs data\n",
    "\n",
    "    test_data =  level_data[test_mask,:]\n",
    "    train_data = level_data[train_mask,:]\n",
    "\n",
    "    #Next generate the features matrix and label matrix to prep for the classification.\n",
    "    labels_test = conditions_multi[test_mask] # choose the category pair and run masks within all the conditions. test labels\n",
    "    features_test = level_data [test_mask,:] # test features\n",
    "\n",
    "    labels_train = conditions_multi[train_mask]\n",
    "    features_train = level_data [train_mask,:]\n",
    "\n",
    "    # Now we can run classification\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "    X_train, X_test = features_train, features_test\n",
    "    y_train, y_test = labels_train, labels_test\n",
    "    \n",
    "    #X_train_scaler =  scaler.fit_transform (X_train)\n",
    "    #X_test_scaler=  scaler.transform (X_test)\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    #model = svm.SVC()\n",
    "    #model = KNeighborsClassifier(n_neighbors=10) \n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "\n",
    "    predicted = model.predict(X_test) # 0:Overcast, 2:Mild\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    \n",
    "    return acc\n",
    "    #train_data.shape\n",
    "\n",
    "    #now we have one run data for testing, we need 9 runs data set for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunc_cross_validation (subject, level, animals):\n",
    "    \"\"\" This function is just the wavefunc with cross-validation built-in\"\"\"   \n",
    "    acc = []\n",
    "    for i in range(10):\n",
    "        acc.append(wavefunc(subject,level,animals,i))\n",
    "\n",
    "    acc =  np.asarray(acc)\n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I would like to loop over all the subjects, all the levels, and all the category pairs.\n",
    "\n",
    "subjects = np.arange(18) # 0-17 ; 18 subjects.\n",
    "\n",
    "levels = np.arange (5) # 0-4; 5 levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each subject:\n",
    "data_all = []\n",
    "\n",
    "for subject in subjects:\n",
    "    for level in levels:\n",
    "            acc = wavefunc_cross_validation(subject, level, animals)\n",
    "            \n",
    "            result_dict = {\n",
    "                \"subject\": subject,\n",
    "                \"level\": level,\n",
    "                \"accuracy\":acc,\n",
    "            }\n",
    "            data_all.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_all)\n",
    "df.to_csv(\"classification_allanimals_VT_xgboost.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04/25, Lastly, I want to try with all the levels information for within vs across. First I tried with scaler ,than no scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 0 \n",
    "\n",
    "#level = 3\n",
    "\n",
    "pair =  [1,2] # represent animal 1 and animal 2 pair\n",
    "\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def wavefunc (subject, pair, run): \n",
    "    \"\"\"This function calculate each subject, each level, each animals pairs, and using leaving one run out cross-validation \n",
    "        method for classification. Subjects are from [0-18]; levels are from 0-4, animals pairs are from 1-12 (two numbers)\n",
    "        , run from 1-10. For example, wavefunc (0,3,[1,2],0)\"\"\"\n",
    "    # First I will start with one subject. \n",
    "\n",
    "    subj_data = data[:,:,:,subject] # First choose one subject data\n",
    "\n",
    "    #level_data = subj_data[:,level,:] # Then choose one level data\n",
    "\n",
    "    #level_data.shape\n",
    "    # choose the masks corresponding to each animal in the pair\n",
    "    mask_1 = conditions_multi == pair[0]\n",
    "    mask_2 = conditions_multi == pair[1]\n",
    "\n",
    "    # Choose one run data according to the animal pair \n",
    "    tr_index = np.arange(1230)\n",
    "    # Each run has 123 TRs, to choose the first run, will use the logic below\n",
    "    run_mask = np.logical_and(run*123 <= tr_index, tr_index < (run+1)*123)\n",
    "\n",
    "    # Select the data by applying the mask\n",
    "    pair_mask = np.logical_or(mask_1, mask_2)\n",
    "    pair_mask = pair_mask.reshape(1230)\n",
    "\n",
    "    test_mask = np.logical_and(pair_mask, run_mask) # choose the animal pair and one run data\n",
    "    train_mask = np.logical_and(pair_mask,~run_mask) # ~ means invert mask, mean except that mask; choose the animal pairs and other 9 runs data\n",
    "\n",
    "    #test_data =  subj_data[test_mask,:,:].reshape(-1,140) #-1 is automatically calcualted based on the number of rows. \n",
    "    #the machine learning has specif structure which is columns represents features and rows represnets labels. \n",
    "    #train_data = subj_data[train_mask,:,:].reshape(-1,140)\n",
    "\n",
    "    #Next generate the features matrix and label matrix to prep for the classification.\n",
    "    labels_test = conditions_multi[test_mask] # choose the animal pair and run masks within all the conditions. test labels\n",
    "    features_test = log(subj_data [test_mask,:,:]).reshape(-1,140) # test features\n",
    "\n",
    "    labels_train = conditions_multi[train_mask]\n",
    "    features_train = log(subj_data [train_mask,:,:]).reshape(-1,140)\n",
    "\n",
    "    # Now we can run classification \n",
    "    # Improvement 04/23 by standard scaling the data which might improve the analysis a little bit.\n",
    "    \n",
    "    #mm_scaler =  preprocessing.MinMaxScaler()\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    \n",
    "\n",
    "    X_train, X_test = features_train, features_test\n",
    "    y_train, y_test = labels_train, labels_test #labels no need to be standard transform\n",
    "    \n",
    "    X_train_scaler =  scaler.fit_transform (X_train)\n",
    "    X_test_scaler=  scaler.transform (X_test)\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    model = svm.SVC()\n",
    "    #model = XGBClassifier()\n",
    "    #model = KNeighborsClassifier(n_neighbors=10)   \n",
    "    model.fit(X_train_scaler,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "\n",
    "    predicted = model.predict(X_test_scaler) # 0:Overcast, 2:Mild\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    \n",
    "    return acc\n",
    "    #train_data.shape\n",
    "\n",
    "    #now we have one run data for testing, we need 9 runs data set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4375\n"
     ]
    }
   ],
   "source": [
    "# Based on my function above, now I would like to run leave one run out cross-validation.\n",
    "acc = []\n",
    "for i in range(10):\n",
    "    acc.append(wavefunc(0,[1,2],i))\n",
    "\n",
    "acc =  np.asarray(acc)\n",
    "print acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunc_cross_validation (subject, pair):\n",
    "    \"\"\" This function is just the wavefunc with cross-validation built-in\"\"\"   \n",
    "    acc = []\n",
    "    for i in range(10):\n",
    "        acc.append(wavefunc(subject,pair,i))\n",
    "\n",
    "    acc =  np.asarray(acc)\n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I would like to loop over all the subjects, and all the animal pairs.\n",
    "\n",
    "subjects = np.arange(18) # 0-17 ; 18 subjects.\n",
    "\n",
    "#levels = np.arange (5) # 0-4; 5 levels.\n",
    "\n",
    "animals = np.arange(1,13)\n",
    "\n",
    "# A Python program to print all \n",
    "# combinations of given list\n",
    "\n",
    "from itertools import combinations \n",
    "\n",
    "# Get all combinations of [1, 2, 3] \n",
    "# and length 2 \n",
    "animal_pairs = list(combinations(animals,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Loop over each subject:\n",
    "data_all = []\n",
    "\n",
    "for subject in subjects:\n",
    "    print subject\n",
    "    #for level in levels:\n",
    "    for animal_pair in animal_pairs:\n",
    "        acc = wavefunc_cross_validation(subject, animal_pair)\n",
    "        if (animal_pair[0]-1)//4 == (animal_pair[1]-1)//4:\n",
    "            within = 1\n",
    "        else:\n",
    "            within = 0\n",
    "        result_dict = {\n",
    "            \"subject\": subject,\n",
    "            #\"level\": level,\n",
    "            \"animal1\":animal_pair[0],\n",
    "            \"animal2\":animal_pair[1],\n",
    "            \"accuracy\":acc,\n",
    "            \"within\":within\n",
    "        }\n",
    "        data_all.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_all)\n",
    "df.to_csv(\"classification_VT_withinvscross_alllevels_svc_logscale_scaler.csv\", index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems the above does not work, then will try will all levels and all animals, then all animals with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 0 \n",
    "\n",
    "#level = 3\n",
    "\n",
    "animals =  [1,2,3,4,5,6,7,8,9,10,11,12] # represent animal 1 and animal 2 pair\n",
    "\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def wavefunc (subject, animals, run): \n",
    "    \"\"\"This function calculate each subject, each level, each animals pairs, and using leaving one run out cross-validation \n",
    "        method for classification. Subjects are from [0-18]; levels are from 0-4, animals pairs are from 1-12 (two numbers)\n",
    "        , run from 1-10. For example, wavefunc (0,3,[1,2],0)\"\"\"\n",
    "    # First I will start with one subject. \n",
    "\n",
    "    subj_data = data[:,:,:,subject] # First choose one subject data\n",
    "\n",
    "    #level_data = subj_data[:,level,:] # Then choose one level data\n",
    "\n",
    "    #level_data.shape\n",
    "    # choose the masks corresponding to each animal in the pair\n",
    "    mask_1 = np.in1d (conditions_multi, animals)\n",
    "    mask_2 = np.in1d (conditions_multi, animals)\n",
    "\n",
    "    # Choose one run data according to the animal pair \n",
    "    tr_index = np.arange(1230)\n",
    "    # Each run has 123 TRs, to choose the first run, will use the logic below\n",
    "    run_mask = np.logical_and(run*123 <= tr_index, tr_index < (run+1)*123)\n",
    "\n",
    "    # Select the data by applying the mask\n",
    "    pair_mask = np.logical_or(mask_1, mask_2)\n",
    "    pair_mask = pair_mask.reshape(1230)\n",
    "\n",
    "    test_mask = np.logical_and(pair_mask, run_mask) # choose the animal pair and one run data\n",
    "    train_mask = np.logical_and(pair_mask,~run_mask) # ~ means invert mask, mean except that mask; choose the animal pairs and other 9 runs data\n",
    "\n",
    "    #test_data =  subj_data[test_mask,:,:].reshape(-1,140) #-1 is automatically calcualted based on the number of rows. \n",
    "    #the machine learning has specif structure which is columns represents features and rows represnets labels. \n",
    "    #train_data = subj_data[train_mask,:,:].reshape(-1,140)\n",
    "\n",
    "    #Next generate the features matrix and label matrix to prep for the classification.\n",
    "    labels_test = conditions_multi[test_mask] # choose the animal pair and run masks within all the conditions. test labels\n",
    "    features_test = subj_data [test_mask,:,:].reshape(-1,140) # test features\n",
    "\n",
    "    labels_train = conditions_multi[train_mask]\n",
    "    features_train = subj_data [train_mask,:,:].reshape(-1,140)\n",
    "\n",
    "    # Now we can run classification \n",
    "    # Improvement 04/23 by standard scaling the data which might improve the analysis a little bit.\n",
    "    \n",
    "    #mm_scaler =  preprocessing.MinMaxScaler()\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    \n",
    "\n",
    "    X_train, X_test = features_train, features_test\n",
    "    y_train, y_test = labels_train, labels_test #labels no need to be standard transform\n",
    "    \n",
    "    #X_train_scaler =  scaler.fit_transform (X_train)\n",
    "    #X_test_scaler=  scaler.transform (X_test)\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    #model = svm.SVC()\n",
    "    model = XGBClassifier()\n",
    "    #model = KNeighborsClassifier(n_neighbors=10)   \n",
    "    model.fit(X_train,y_train)# model fit traning set.\n",
    "\n",
    "    #Predict Output\n",
    "\n",
    "    predicted = model.predict(X_test) # 0:Overcast, 2:Mild\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    \n",
    "    return acc\n",
    "    #train_data.shape\n",
    "\n",
    "    #now we have one run data for testing, we need 9 runs data set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunc_cross_validation (subject, animals):\n",
    "    \"\"\" This function is just the wavefunc with cross-validation built-in\"\"\"   \n",
    "    acc = []\n",
    "    for i in range(10):\n",
    "        acc.append(wavefunc(subject,animals,i))\n",
    "\n",
    "    acc =  np.asarray(acc)\n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I would like to loop over all the subjects, and all the animal pairs.\n",
    "\n",
    "subjects = np.arange(18) # 0-17 ; 18 subjects.\n",
    "\n",
    "#levels = np.arange (5) # 0-4; 5 levels.\n",
    "\n",
    "animals = np.arange(1,13)\n",
    "\n",
    "# A Python program to print all \n",
    "# combinations of given list\n",
    "\n",
    "from itertools import combinations \n",
    "\n",
    "# Get all combinations of [1, 2, 3] \n",
    "# and length 2 \n",
    "#animal_pairs = list(combinations(animals,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Loop over each subject:\n",
    "data_all = []\n",
    "\n",
    "for subject in subjects:\n",
    "    print subject\n",
    "    #for level in levels:\n",
    "    #for animal in animals:\n",
    "    acc = wavefunc_cross_validation(subject, animals)\n",
    "    #if (animal_pair[0]-1)//4 == (animal_pair[1]-1)//4:\n",
    "        #within = 1\n",
    "    #else:\n",
    "        #within = 0\n",
    "    result_dict = {\n",
    "        \"subject\": subject,\n",
    "        #\"level\": level,\n",
    "        #\"animal1\":animal_pair[0],\n",
    "        #\"animal2\":animal_pair[1],\n",
    "        \"accuracy\":acc,\n",
    "        #\"within\":within\n",
    "    }\n",
    "    data_all.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_all)\n",
    "df.to_csv(\"classification_VT_alllevels_allanimals_xgboost.csv\", index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
